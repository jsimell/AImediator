# Task template for use with the Collaborative AI Arena

This module serves as a template task ( and as a suggestion on how to set up communication pathes)
The aim of this template is to allow concurrent processing of requests and implement the flow for the Task component as detailed in the following diagram:

![A diagram showing the flow of messages from the model perspective](docs/threecomp_layout.svg)

## Task Messages

There are four types of messages, a task will receive or send, which are all defined by the `tasks.proto` file.
The messages along with their fields are:

- Incoming Messages
  - `modelAnswer`:
    - `answer` : A string represnting a json object with the following fields:
      - `text` : The text of the answer of the model
      - `image`: a base64 encoded image.
    - `sessionID`: An ID of the session that this response is for
    - `messageID`: An ID of the individual message sent, to be able to answer to this message
  - `modelInfo`:
    - `modelName` : The name of the model
    - `sessionID`: An ID of the session that this info is for
- Outgoing Messages:
  - `taskRequest`
    - `request` : A string representing a json object with messages in OpenAI message format, including a system message.
    - `sessionID`: An ID of the session that generated this resource
    - `messageID`: An ID of the individual message sent, to be able to answer to this message
  - `taskMetrics`:
    - `metrics`: A String that is alredy formatted and just needs to be put into the models log, so that it can be interpreted by AIBuilder for the leaderboard
    - `sessionID`: The session for which these metrics were generated
  - `modelRequirements`: A message that indicates, what the model needs to be able to handle for this session of this task
    - `needs_text`: A Bool indicating whether the model needs text for processing
    - `needs_image`: A Bool indicating whether the model needs an image for processing
    - `sessionID`: The ID for these requirements

## Task Services

The task will need to implement the four services detailed in the tasks.proto file.
Those contain:

- `startTask` : A Service which is called with an empty request on startup and needs to inform the model handler, whenever a new session is generated by the task (i.e. for each new run of a task). The message needs to define the `modelRequirements` containing the `sessionID`, which will allow the model handler to select one model for this session. This definition indicates the capabilities of the model required to fullfil the task (i.e. it needs to specify whether the model needs to be able to handle text and images to work with this task).
- `runTask`: This is again a service with an empty request, which needs to provide a sstream of `taskRequest` messages, which will be allocated to the correct model by the model handler.
- `finishTask`: Again a service that needs to provide a stream of `taskMetrics` messages upon completion of the task by the user.
- `getModelResponse`: This service receives the responses by the model, and needs to be able to handle them and assign them to the web-requests that caused their generation.
- `receiveModelInfo`: This service receives information about the model used once a job is finished and can thus inform the frontend which model has been used.

The logic of the processing is as follows:
When a user sends a request that they want to start the task, the `startTask` service should emit a `modelRequirements` message. This message
is used by the model handler to select a model for the indicated session.
After this message, the `runTask` service can emit several `taskRequest` messages, which will be processed and forwarded by the model handler
to the appropriate model.
Finally, when a session (i.e. running one task) ends, the task needs to emit one `taskMetrics` message from the `finishTask` service to indicate
that this session is over, and receives a new message at the receiveModelInfo service about which modl has been used. Then the association with the session can be removed. After that it can emit a new `modelRequirements` message from the `startTask`
service, if a new session was started etc..

## Implementation

We provide a sample implementation of a Task Server in the `task_server.py` class.
The `main.py` file further implements a FastAPI server which can serve a ready to serve front-end along with three end-points that can be used as
a basis and implementation example for tasks and the communication logic.

### API Models:

We allow two different types of processing that allow for different features to be used based on a tasks preferences.
One allows automatic addition of history (text) messages to all conversations. This is the "basic" version. The other is more openAI like in that it allows generating and processing message arrays. For security reasons, in both instances, the back-end and not the front-end has to handle the setting of the system prompt. This is necessary to ensure, that the endpoints we offer cannot be used by others as "free" interfaces to the models, but that what the model does is always based on the system prompts that are given for a task.

We have defined a few pydantic models that we use for interaction between the model server and the actual model implementation in the `models.py` module.

**The models for response processing and definition of the tasks requirements is the same in both task variants**:

- `ModelResponse`, The response obtained by the model. It contains text and image data, depending on the model and requirements of the task

  - `text` : a String with the model answer
  - `image` : a string base64 encoded image of the model answer (can be None)

- `TaskDataResponse`, The response obtained by the model. It contains text and image data, depending on the model and requirements of the task

  - `text` : a String with the model answer
  - `image` : a string base64 encoded image of the model answer (can be None)
  - `outputData`: as inputdata this is a free form field, which can contain any json encoded data to make it easier for the user to pass data back to the front-end

- `TaskRequirements`, A definition of the requirements for this task

  - `needs_text` : Whether the task needs a model that can handle text
  - `needs_image` : Whether the task needs a model that can handle images

**For basic request building the following models are relevant**

- `TaskDataRequest`, this is the incoming data request from the front end and needs to be interpreted by the task code.

  - `text` : An optional field for textual input (for convenience)
  - `image`: An optional field containing an image in base64 representation
  - `objective` : An optional field for a user defined objective to the task (for convenience)
  - `inputData`: A required field containing any JSON formatted data, which the task needs to know about. This can also be empty, if the task only handles the text or image fields, but is a catch all to allow the user to provide their specific data.

Standard processing will only allow provision of one message at a time. This approach will automatically keep a history of messages, so it's important to avoid concurrent messages as they might otherwise share history. If you need to run concurrent messages, we suggest using the openAI styled interface.

- `TaskRequest`, A class that the task needs to provide to the server.
  - `text` : The text (user input) which is provided to the model as the user message. This needs to be generated from the input data.
  - `image` : a string base64 encoded image that the user provided
  - `system` : The system message describing the task to solve

**For openAI like request handling the following models are relevant for request generation**

- `OpenAIBasedDataRequest`, A data request with messages similar to openAI messages. System messages are nto allowed in this request.
  - `userMessages` : an openAI like message array with messages defined similar to openAI. Currently allows Image and Text messages. Allowed roles include "user", "assistant" and "tool". "system" is not an allowed role in the input messages.
  - `objective` : convenience `str` field that can hold an objective which can be inserted into the system message
  - `inputData` : Free form input data that can be used to build system messages or additional information in the back-end.

The idea of splitting is to allow the frontend to keep track of the history of messages, while having a `inputData` field that can be used to fill in the relevant information to the system message.

- `OpenAIBasedRequest`, similar to the `TaskRequest` this is the request generated by the actual task from the `OpenAIBasedDataRequest`. It only contains the messages that will be sent on to the model
  - `messages` : an openAI like message array (including a system message)

### Interfaces

The server handles most functionality on the back-end side. It requires the implementation of the field task in the `tasks/task.py` file which needs to implement either the `Task` or `OpenAITask` interface from `tasks/task_interface.py`.
The following methods need to be implemented for the two different types of tasks:

- Both:

  - `get_requirements() -> TaskRequirements`
    - A function that needs to return the requirements of the Task, to be able to select the right model.

- **Standard Task**:

  - `generate_model_request(request: TaskDataRequest)-> TaskRequest`
    - Generate the model Request from the input data. the request.inputData field is a free-form json, that you can use for front-end back-end communication for any "non standard" data transfer
  - `process_model_answer( answer: ModelResponse) -> TaskDataResponse`
    - process the model response. E.g. if you have given a specific format to the models and need to parse that format for your front-end

- **OpenAI Task**:

  - `generate_model_request_openAI(request: OpenAIBasedDataRequest)-> OpenAIBasedRequest`
    - Generate the model Request from the input data. the request.inputData field is a free-form json, that you can use for front-end to back-end communication for any "non standard" data transfer
  - `process_model_answer_openAI( answer: ModelResponse) -> TaskDataResponse`
    - process the model response. E.g. if you have given a specific format to the models and need to parse that format for your front-end

#### Front-end

The current docker file assumes a frontend folder which contains builds the `frontend` into it's `dist` folder. The server will serve these files automatically.
This might need to be changed for your front-end but generally any compiled front-end can be used.
If you use a different front-end tech to the one in this project, the compiled frontend needs to be placed into the "dist" folder from which it will be served by the FastAPI server. Also make sure, that your frontend does not rely on pathes that are part of the backend server.

#### Frontend-backend interaction

The back-end provides two endpoints at the moment:

- `/api/v1/task/process/`, which expects a `TaskDataRequest` object to be processed.
  This endpoint will respond with a `TaskDataResponse`, with the contents depending on the response of the model and the post-processing done in the `process_model_answer` function of the task. (This endpoint is for tasks implementing the `Task` interface)
- `/api/v1/task/completions/`, which expects a `OpenAIBasedDataRequest` object to be processed.
  This endpoint will respond with a `TaskDataResponse`, with the contents depending on the response of the model and the post-processing done in the `process_model_answer_openAI` function of the task. (This endpoint is for tasks implementing the `OpenAITask` interface)
- `/api/v1/task/finish/` this endpoint indicates that one session has finished, and expects a `TaskMetrics` object that is passed on to the model handler, supplemented with the session ID. This will also clear the session association held by the back end, and the triggered model handler call will clear out the model associated with the current session.
  The response will contain the name of the used model.

### Front-end templates

Three examples currently exist for the frontend which are roughly similar.
One is for a poetry interaction, where the user and the LLM create a poem together.
The second is for a tangram game, where the user can place pieces and the AI can inform them of where to put pieces.
The third is a more complex frontend for a gesture task where the user's camera is used to peform actions that are to be interpreted by the AI.
The two different tasks have individual docker files (`Dockerfile_poetry` and `Dockerfile_tangram` respectively)

In addition, we provide a general react-based template for integration into the AI Arena. (in `/react_task_template`). This template can be used to either include another compiled frontend via an iframe (the tangram game is an example for this integration), or to build additional components in the frontend. The important parts of this template are the header, footer and Feedback, for which the structure is included in the `src/App.jsx` component. This structure is important to achieve a common look and feel for all tasks, and your task should be "within" this.
